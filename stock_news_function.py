import os
import json
import pandas as pd
from bs4 import BeautifulSoup
from selenium import webdriver
import time
from selenium.webdriver.chrome.options import Options

def  stock_news():
    # 1. 셀레니움으로 페이지 열기
    chrome_options = Options()
    chrome_options.add_argument("--headless")  # 브라우저 창 없이 실행
    driver = webdriver.Chrome(options=chrome_options)
    driver.get("https://finance.daum.net/news#stock")
    time.sleep(2)  # 페이지 렌더링 대기

    html = driver.page_source
    soup = BeautifulSoup(html, 'html.parser')
    driver.quit()

    # 2. 뉴스 항목 찾기
    news_items = soup.find_all('li', class_='imgB')

    # 3. 뉴스 데이터를 리스트로 수집
    news_data = []
    for item in news_items:
        title_tag = item.find('a', class_='tit')
        title = title_tag.text.strip() if title_tag else ''

        text_tag = item.find('a', class_='txt')
        text = text_tag.text.strip() if text_tag else ''

        article_url = title_tag['href'] if title_tag else ''

        img_tag = item.find('img')
        img_url = img_tag['src'] if img_tag else ''

        date_tag = item.find('p', class_='date')
        source, date = '', ''
        if date_tag:
            parts = date_tag.text.strip().split('·')
            if len(parts) == 2:
                source = parts[0].strip()
                date = parts[1].strip()

        news_data.append({
            'title': title,
            'summary': text,
            'imageURL': img_url,
            'newsURL': article_url,
            'newsAgency': source,
            'date': date
        })

    # 4. DataFrame 생성
    df = pd.DataFrame(news_data)

    # 5. JSON으로 저장
    base_dir = os.path.dirname(os.path.abspath(__file__))

    # 같은 디렉토리에 JSON 저장
    json_path = os.path.join(base_dir, "stock_news.json")
    df.to_json(json_path, orient='records', force_ascii=False, indent=4)

    json_str = df.to_json(orient='records', force_ascii=False, indent=4)
    # 👉 JSON 문자열을 파이썬 객체로 로드해서 리턴
    return json.loads(json_str)
